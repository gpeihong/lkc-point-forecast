{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a344836-2055-4318-b7ca-62f093c5cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 Individual forecasting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f433e-8aff-4fcb-8a4b-697e5b57f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "weatherclimateED = pd.read_csv('weatherclimateED.csv', parse_dates = [0], dayfirst = True)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d1977-f012-4005-9a58-dd6b010c27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Epiweeks Module converts dates to CDC Epiweek format\n",
    "## Further documentation on https://pypi.org/project/epiweeks/\n",
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "def create_epiweek(date):\n",
    "    return Week.fromdate(date)\n",
    "def create_epiweekplot(epiweek):\n",
    "    epiweek = str(epiweek)\n",
    "    return F'Y{epiweek[:4]}W{epiweek[4:]}'\n",
    "def create_epiweek_fromstr(str):\n",
    "    return Week.fromstring(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f4c3f-fbee-4b7f-8197-c3948ab6c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This section creates a full complete dataset that includes all the variables of interest that will be used\n",
    "## iloc function selects the relevant variables of interest based on column number\n",
    "## Problematic weather columns (i.e. don't select!): 6, 16, 17, 19, 20\n",
    "## Disease columns excluded due to limited dataset: 21:24, 25\n",
    "weatherclimateED['Date'] = pd.to_datetime(weatherclimateED['Date'])\n",
    "weatherclimateED['epiweek'] = weatherclimateED['Date'].apply(create_epiweek)\n",
    "weatherclimateED = weatherclimateED.set_index('epiweek')\n",
    "weatherclimateED = weatherclimateED.iloc[:, np.r_[30:32, 33:39, 40, 42 , 45:47, 49:51,  52:54, 1:6, 8:15]]\n",
    "weatherclimateED.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f3296-097d-40b0-a7c2-6d598a519f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This function takes the full dataset and creates an initial dataset with the specified range\n",
    "## also returns the name of the target variable for creation of the initial dataset\n",
    "## note disease_var here is an integer based off the column number\n",
    "def create_initial_dataset(dataset, disease_var: int):\n",
    "    explore_df = dataset.copy()\n",
    "    range_start = Week(2009,1)\n",
    "    range_end = Week (2018,52)\n",
    "    explore_df = explore_df.loc[range_start:range_end]\n",
    "    target_var = explore_df.columns.values.tolist()[disease_var]\n",
    "\n",
    "    if not os.path.exists(target_var):\n",
    "        os.makedirs(target_var)\n",
    "    path = os.path.join(target_var, F'initial_dataset.csv')\n",
    "    \n",
    "    explore_df.to_csv(path)\n",
    "    #explore_df1 is pure AR and explore_df2 is with environmetal vairables\n",
    "    explore_df_1 = explore_df[[target_var]] \n",
    "    explore_df_2 = pd.merge(explore_df[[target_var]], explore_df[explore_df.columns[16:28].to_list()], on='epiweek')\n",
    "#     explore_df_pure = explore_df.drop(columns=target_var)\n",
    "    return explore_df, explore_df_1, explore_df_2, target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc107e1c-6d9b-4139-a100-b1726b6162b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_naive(dataset, step, target_var):\n",
    "    naive = dataset.copy()\n",
    "    naive = naive[[target_var]].shift(step)\n",
    "    return naive.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_history_mean(dataset, lag, step, target_var):\n",
    "    origin_history_mean = dataset.copy()\n",
    "    history_mean = origin_history_mean[[target_var]].shift(step)\n",
    "    for i in range(step + 1, step + lag):\n",
    "        history_mean += origin_history_mean.shift(i)\n",
    "    return history_mean.dropna() / lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b1deb-e97f-45d7-b5f7-6c4f296caea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged dataset\n",
    "def create_lagged_dataset(dataset, lag, target_var):\n",
    "    lagged_dataset = dataset.copy()\n",
    "    columns_list = list(lagged_dataset.columns)\n",
    "    data_join = {}\n",
    "    for column in columns_list:\n",
    "        if column == target_var:\n",
    "            data_join[column] = lagged_dataset[column]\n",
    "        for n in range(1,lag+1):\n",
    "            data_join[F'{column}_L{n}'] = lagged_dataset[column].shift(n)\n",
    "    lagged_dataset = pd.concat(data_join.values(), axis=1, ignore_index = True)\n",
    "    lagged_dataset.columns = data_join.keys()\n",
    "    return lagged_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672d89c-abc7-4fa5-8e1b-5c6bfc8fffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step is the number of weeks ahead that we are forecasting, e.g. step=2 is 2 weeks ahead.\n",
    "## Note step=1 results in no change to dataset, i.e. use generated lagged variables to forecast current. \n",
    "def create_stepped_dataset(dataset, step, target_var):\n",
    "    stepped_dataset = dataset.copy()\n",
    "    y = stepped_dataset[[target_var]].shift(-step+1)\n",
    "    if step != 1:\n",
    "        X = stepped_dataset.iloc[:-step+1, :]\n",
    "    else:\n",
    "        X = stepped_dataset\n",
    "    return X.drop(target_var, axis = 1), y.dropna()\n",
    "## So now target variable (y variable for exploration) is shifted back by 2 weeks. i.e., taking the y-value from 2 weeks later\n",
    "## and setting it to the current index. So linear regression of y+2 with the current X values. X will have\n",
    "## a smaller dataset with the last 2 time points removed because of the shift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112b902-bf3c-426a-ac21-d98a3bd3a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(X, window_perc):\n",
    "    return X.index[0], X.index[int(len(X)*window_perc)]\n",
    "def create_output_dataset(y, window_end):\n",
    "    return y.copy().loc[window_end+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f8bdf-0d86-42b3-a937-6094d334ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from group_lasso import GroupLasso\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "# from nixtlats import TimeGPT\n",
    "from sklearn.decomposition import PCA\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import globalenv\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import Formula\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "def errors(model, model_name, X_train, y_train, errors_path, filename, pred_train_path):\n",
    "    if model_name == 'naive':\n",
    "        y_train_errors = pd.DataFrame(X_train - y_train)\n",
    "        y_pred_train = pd.DataFrame(X_train)\n",
    "    else:\n",
    "        y_train_errors = pd.DataFrame(model.predict(X_train) - y_train)\n",
    "        y_pred_train = pd.DataFrame(model.predict(X_train))\n",
    "    #print(y_train_errors)\n",
    "    #y_train_errors will contain list of errors in training set from window_start to window_end\n",
    "    #right before window_end+1 in the filename\n",
    "    errors_path = os.path.join(errors_path, model_name)\n",
    "    if not os.path.exists(errors_path):\n",
    "        os.makedirs(errors_path)\n",
    "    y_train_errors_path = os.path.join(errors_path, F'{filename}.csv')\n",
    "    y_train_errors.to_csv(y_train_errors_path)\n",
    "\n",
    "    pred_train_path = os.path.join(pred_train_path, model_name)\n",
    "    if not os.path.exists(pred_train_path):\n",
    "        os.makedirs(pred_train_path)\n",
    "    y_pred_train_path = os.path.join(pred_train_path, F'{filename}.csv')\n",
    "    y_pred_train.to_csv(y_pred_train_path)\n",
    "\n",
    "def coefs(model, coefs_path, filename):\n",
    "    coefs_path\n",
    "\n",
    "## This function runs the first order regression for the target disease, for one specified lag and step\n",
    "\n",
    "def regression_with_naive(X_dataset, y_dataset, X_dataset_1, y_dataset_1, X_dataset_2, y_dataset_2, window_start, window_end, y_pred, y_params, errors_path, test_length, naive, history_mean, target_var, y_ridge, y_lasso, pred_train_path, lag):\n",
    "    count = 0\n",
    "    df_end = X_dataset.index[-1]\n",
    "    while window_end != df_end:\n",
    "        X = X_dataset.copy()\n",
    "        y = y_dataset.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train = X.loc[window_start:window_end]\n",
    "        #print(X_train.info())\n",
    "        ## values.ravel() converts y_train to numpy array for compatibility with models (update: already deleted this)\n",
    "        y_train = y.loc[window_start:window_end]\n",
    "        #print(len(y_train))\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test = X.loc[[window_end+1]]\n",
    "        #print(X_test)\n",
    "        y_test = y.loc[window_end+1]\n",
    "        #print(y_test)\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # For all models using all variables, I use dataframe and not array\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.columns = X.columns\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = X.columns\n",
    "\n",
    "        \n",
    "        ## data processing for pure AR \n",
    "        X_1 = X_dataset_1.copy()\n",
    "        y_1 = y_dataset_1.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train_1 = X_1.loc[window_start:window_end]\n",
    "        y_train_1 = y_1.loc[window_start:window_end]\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test_1 = X_1.loc[[window_end+1]]\n",
    "        y_test_1 = y_1.loc[window_end+1]\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train_1 = scaler.fit_transform(X_train_1)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test_1 = scaler.transform(X_test_1)\n",
    "        # For pure factor model, I use dataframe and not array\n",
    "        X_train_1 = pd.DataFrame(X_train_1)\n",
    "        X_train_1.columns = X_1.columns\n",
    "        X_test_1 = pd.DataFrame(X_test_1)\n",
    "        X_test_1.columns = X_1.columns\n",
    "        \n",
    "        ## data processing for AR with environmental variables\n",
    "        X_2 = X_dataset_2.copy()\n",
    "        y_2 = y_dataset_2.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train_2 = X_2.loc[window_start:window_end]\n",
    "        y_train_2 = y_2.loc[window_start:window_end]\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test_2 = X_2.loc[[window_end+1]]\n",
    "        y_test_2 = y_2.loc[window_end+1]\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train_2 = scaler.fit_transform(X_train_2)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test_2 = scaler.transform(X_test_2)\n",
    "    \n",
    "        ## evaluate variance\n",
    "        \n",
    "        ## Implement cross-validation split\n",
    "        tscv = TimeSeriesSplit(n_splits = 5)\n",
    "        \n",
    "    \n",
    "        ## 1. Naive Forecast\n",
    "        # add the [0] to extract as float, and not as series\n",
    "        y_pred.at[window_end+1, 'naive'] = naive.loc[window_end+1][0]\n",
    "\n",
    "#         errors(naive, 'naive', naive.loc[window_start:window_end].values.ravel(), y_train, errors_path, window_end+1, pred_train_path)\n",
    "        \n",
    "        ## 2. Historical mean (rolling window = lag)\n",
    "        y_pred.at[window_end+1, 'historymean'] = history_mean.loc[window_end+1][0]\n",
    "\n",
    "        ## 3. Pure AR\n",
    "        ar_pure = LinearRegression()\n",
    "        ar_pure.fit(X_train_1, y_train_1)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'ar_pure'] = ar_pure.predict(X_test_1)\n",
    "        \n",
    "        ## 4. AR only with environmental variables\n",
    "        ar_env = LinearRegression()\n",
    "        ar_env.fit(X_train_2, y_train_2)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'ar_env'] = ar_env.predict(X_test_2)\n",
    "        \n",
    "        ## 5. AR with all variables\n",
    "        ar_all = LinearRegression()\n",
    "        ar_all.fit(X_train, y_train)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'ar_all'] = ar_all.predict(X_test)\n",
    "\n",
    "#         errors(linreg_model, 'linreg', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "        \n",
    "    \n",
    "        ## 6. Ridge model\n",
    "        ridge_cv = RidgeCV(cv = tscv)\n",
    "        ridge_cv.fit(X_train, y_train)\n",
    "    \n",
    "        ridge_model = Ridge(alpha = ridge_cv.alpha_)\n",
    "        ridge_model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred.at[window_end+1, 'ridge'] = ridge_model.predict(X_test)\n",
    "        y_params.at[window_end+1, 'ridge_alpha'] = ridge_cv.alpha_\n",
    "\n",
    "\n",
    "#         alpha = ridge_cv.alpha_\n",
    "#         ridge_edf = 0\n",
    "#         for d in ridge_model.coef_:\n",
    "#             ridge_edf += d**2/(d**2+alpha)\n",
    "        \n",
    "#         y_ridge.at[window_end+1, 'ridge_edf'] = ridge_edf\n",
    "        \n",
    "#         errors(ridge_model, 'ridge', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "\n",
    "        \n",
    "        ## 7. Lasso Model\n",
    "        lasso_cv = LassoCV(cv = tscv, random_state = 18, max_iter = 100000)\n",
    "        lasso_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Create the Lasso model with the optimal alpha value\n",
    "        lasso_model = Lasso(alpha = lasso_cv.alpha_)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'lasso'] = lasso_model.predict(X_test)\n",
    "        y_params.at[window_end+1, 'lasso_alpha'] = lasso_cv.alpha_\n",
    "        y_params.at[window_end+1, 'lasso_n_iter'] = lasso_cv.n_iter_\n",
    "        \n",
    "#        y_lasso.at[window_end+1, 'lasso_edf'] = np.count_nonzero(lasso_model.coef_)\n",
    "\n",
    "#         errors(lasso_model, 'lasso', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "        \n",
    "        ## 8. Adaptive Lasso regression\n",
    "        linear_reg = LinearRegression()\n",
    "        linear_reg.fit(X_train, y_train)\n",
    "        initial_coef = linear_reg.coef_\n",
    "        # Calculate weights for the adaptive Lasso\n",
    "        weights = 1 / (np.abs(initial_coef) + 1e-5)\n",
    "        X_train_weighted = X_train / weights\n",
    "        \n",
    "        lasso_adaptive = Lasso(alpha = lasso_cv.alpha_)\n",
    "        lasso_adaptive.fit(X_train_weighted, y_train)\n",
    "        \n",
    "        lasso_adaptive.coef_ = lasso_adaptive.coef_ / weights\n",
    "        y_pred.at[window_end+1, 'alasso'] = lasso_adaptive.predict(X_test)\n",
    "        \n",
    "        \n",
    "        ## 9. Group Lasso regression\n",
    "        group_sizes = [8 for i in range(int(X_train.shape[1]/lag))]\n",
    "        groups = np.concatenate(\n",
    "            [size * [i] for i, size in enumerate(group_sizes)]\n",
    "        ).reshape(-1, 1)\n",
    "        \n",
    "        lambda_values = 10.0 ** np.arange(-1, 1, 0.3)  # Adjust according to your needs\n",
    "        alpha_values = np.arange(0, 1, 0.4)\n",
    "\n",
    "        # Placeholder for storing error for each lambda value\n",
    "        errors = np.zeros((len(lambda_values), len(alpha_values)))\n",
    "\n",
    "        # Loop over each lambda value\n",
    "        for i, lambda_val in enumerate(lambda_values):\n",
    "            for j, alpha_val in enumerate(alpha_values):\n",
    "                temp_errors = []\n",
    "                for train_index_cv, test_index_cv in tscv.split(X_train):\n",
    "                    X_train_cv, X_test_cv = X_train.iloc[train_index_cv], X_train.iloc[test_index_cv]\n",
    "                    y_train_cv, y_test_cv = y_train.iloc[train_index_cv], y_train.iloc[test_index_cv]\n",
    "\n",
    "                    # Initialize and fit the GroupLasso model\n",
    "                    model = GroupLasso(groups=groups, group_reg=lambda_val, l1_reg=alpha_val, random_state=18, scale_reg=\"inverse_group_size\", fit_intercept=True, n_iter=100000, supress_warning=True)\n",
    "                    model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "                    # Predict and calculate MSE for this fold\n",
    "                    y_pred_cv = model.predict(X_test_cv)\n",
    "                    mse = mean_squared_error(y_test_cv, y_pred_cv)\n",
    "                    temp_errors.append(mse)\n",
    "\n",
    "                # Average MSE across all folds for this lambda\n",
    "                errors[i, j] = np.mean(temp_errors)\n",
    "\n",
    "        # Find the lambda value and alpha value with the lowest error\n",
    "        min_error_idx = np.unravel_index(errors.argmin(), errors.shape)\n",
    "        best_lambda = lambda_values[min_error_idx[0]]\n",
    "        best_alpha = alpha_values[min_error_idx[1]]\n",
    "        \n",
    "        # Create the sgl model with the optimal lambda and alpha value\n",
    "        sgl_model = GroupLasso(groups=groups, group_reg=best_lambda, l1_reg=best_alpha, random_state=18, scale_reg=\"inverse_group_size\", fit_intercept=True, n_iter=100000, supress_warning=True)\n",
    "        sgl_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'sgl'] = sgl_model.predict(X_test)\n",
    "        y_params.at[window_end+1, 'sgl_lambda'] = best_lambda\n",
    "        y_params.at[window_end+1, 'sgl_alpha'] = best_alpha\n",
    "        \n",
    "        \n",
    "        ## 10. ElasticNet Model\n",
    "        elasticnet_cv = ElasticNetCV(cv = tscv, max_iter = 100000)\n",
    "        elasticnet_cv.fit(X_train, y_train)\n",
    "    \n",
    "        # Create the ElasticNet model with the optimal l1 and alpha values\n",
    "        elasticnet_model = ElasticNet(alpha = elasticnet_cv.alpha_, l1_ratio = elasticnet_cv.l1_ratio_)\n",
    "        elasticnet_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'elasticnet'] = elasticnet_model.predict(X_test)\n",
    "        \n",
    "        y_params.at[window_end+1, 'elasticnet_alpha'] = elasticnet_cv.alpha_\n",
    "        y_params.at[window_end+1, 'elasticnet_l1ratio'] = elasticnet_cv.l1_ratio_\n",
    "\n",
    "#         errors(elasticnet_model, 'elasticnet', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "        \n",
    "        ## 11. Adaptive ElasticNet Model\n",
    "        anent_model = LinearRegression() # first fit a normal lm\n",
    "        anent_model.fit(X_train, y_train)\n",
    "        \n",
    "        pandas2ri.activate()\n",
    "\n",
    "        glmnet = importr('glmnet')\n",
    "        base = importr('base')\n",
    "        base.set_seed(123)\n",
    "\n",
    "        X_r_train = pandas2ri.py2rpy(X_train)\n",
    "        y_r_train = pandas2ri.py2rpy(y_train)\n",
    "\n",
    "        # Convert to matrices in R\n",
    "        ro.r.assign('X_r_train', X_r_train)\n",
    "        ro.r.assign('y_r_train', y_r_train)\n",
    "        ro.r('X_r_train <- as.matrix(X_r_train)')\n",
    "        ro.r('y_r_train <- as.matrix(y_r_train)')\n",
    "\n",
    "        # Fit a linear model (without intercept, adjust according to your needs)\n",
    "        ro.r('model <- lm(y_r_train ~ X_r_train)')  # Adjusted to omit the intercept\n",
    "\n",
    "        # Extract coefficients\n",
    "        ro.r('coefficients <- as.numeric(coef(model))[-1]')  # Adjust if intercept is included\n",
    "\n",
    "        # Perform adaptive LASSO with cross-validation\n",
    "        ro.r('''\n",
    "        aenet_cv <- cv.glmnet(x = X_r_train, y = y_r_train,\n",
    "                                type.measure = \"mse\",\n",
    "                                nfold = 10,\n",
    "                                alpha = 0.5,\n",
    "                                penalty.factor = 1 / (abs(coefficients) + 1e-5),\n",
    "                                keep = TRUE)\n",
    "        ''')\n",
    "\n",
    "        # Extract best coefficients\n",
    "        ro.r('best_aenet_coef <- coef(aenet_cv, s = aenet_cv$lambda.min)')\n",
    "        ro.r('best_aenet_coef <- as.numeric(best_aenet_coef)')\n",
    "\n",
    "        # Retrieve the best coefficients back into Python\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            best_aenet_coef = ro.conversion.rpy2py(ro.globalenv['best_aenet_coef'])\n",
    "        \n",
    "        anent_model.coef_ = best_aenet_coef[1:].reshape(1, X_train.shape[1])\n",
    "        anent_model.intercept_ = np.array([best_aenet_coef[0]])\n",
    "        y_pred.at[window_end+1, 'aenet'] = anent_model.predict(X_test)\n",
    "        \n",
    "        ## 12. Pure factor model\n",
    "        print(X_train)\n",
    "        remove_names = []\n",
    "        for name in X_train.columns:\n",
    "            if name[0:-3] == target_var:\n",
    "                remove_names.append(name)\n",
    "\n",
    "        X_train_pure = X_train.drop(columns=remove_names)\n",
    "        X_test_pure = X_test.drop(columns=remove_names)\n",
    "        pca = PCA()\n",
    "        pca.fit(X_train_pure)\n",
    "        cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "          #to explain more than 85% of the variance\n",
    "        num_components = np.where(cumulative_variance_ratio >= 0.85)[0][0] + 1 \n",
    "        pca_new = PCA(n_components=num_components)\n",
    "        X_train_pca = pca_new.fit_transform(X_train_pure)\n",
    "        X_train_pca = pd.DataFrame(X_train_pca)\n",
    "        X_train_pca.columns = X_train_pca.columns.astype(str)\n",
    "        X_train_pca = pd.merge(X_train_1, X_train_pca, left_index=True, right_index=True)\n",
    "\n",
    "        X_test_pca = pca_new.transform(X_test_pure)\n",
    "        X_test_pca = pd.DataFrame(X_test_pca)\n",
    "        X_test_pca.columns = X_test_pca.columns.astype(str)\n",
    "        X_test_pca = pd.merge(X_test_1, X_test_pca, left_index=True, right_index=True)\n",
    "        \n",
    "        pure_factor_model = LinearRegression()\n",
    "        pure_factor_model.fit(X_train_pca, y_train)\n",
    "        y_pred.at[window_end+1, 'purefactor'] = pure_factor_model.predict(X_test_pca)\n",
    "        \n",
    "        ## 13. Random Forest\n",
    "        randomforest_model = RandomForestRegressor(n_estimators = 1000, max_features = 'sqrt', random_state = 18)\n",
    "        randomforest_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'randomforest'] = randomforest_model.predict(X_test)\n",
    "#         errors(randomforest_model, 'randomforest', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "    \n",
    "    \n",
    "        ## 14. KNN\n",
    "        knn_model = KNeighborsRegressor() #  default parameters\n",
    "        knn_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'knn'] = knn_model.predict(X_test)\n",
    "#         errors(knn_model, 'knn', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "\n",
    "\n",
    "        ## 15. XGBoost\n",
    "        xgboost_model = xgb.XGBRegressor(n_estimators=1000, random_state=18)\n",
    "        xgboost_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'xgboost'] = xgboost_model.predict(X_test)\n",
    "#         errors(gradientboost_model, 'gradientboost', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "\n",
    "\n",
    "        ## 16. LightGBM\n",
    "        lightgbm_model = lgb.LGBMRegressor(objective='regression', n_estimators= 1000, random_state=18, verbosity=-1)\n",
    "        lightgbm_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'lightgbm'] = lightgbm_model.predict(X_test)\n",
    "#         errors(gradientboost_model, 'gradientboost', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "        \n",
    "    \n",
    "        ##\n",
    "        #keep track of model progress, every number of weeks\n",
    "        tracking_interval = 5\n",
    "        if window_end.weektuple()[1] % tracking_interval == 0:\n",
    "            print(F'{target_var} done with {window_end+1}; {count} out of {test_length}')\n",
    "            \n",
    "        ## Implement expanding window\n",
    "        #window_start = window_start+1 (only for rolling window)\n",
    "        window_end += 1\n",
    "        count += 1\n",
    "\n",
    "    print(F'The last epiweek for {target_var} to be predicted is: {window_end}')\n",
    "    print(F'The total number of predicted epiweeks for {target_var} is: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12edf56-31b5-4b42-a957-7df729f4892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function sets up the first order regression for the target disease, for one specified lag and step\n",
    "\n",
    "def run_first_order_regression(dataset, dataset_1, dataset_2, lag, step, target_var, window_perc):\n",
    "    print(F'Running first order regression for {target_var} lag {lag} step {step}')\n",
    "    \n",
    "    naive = create_naive(dataset, step, target_var)\n",
    "    history_mean = create_history_mean(dataset, lag, step, target_var)\n",
    "    \n",
    "    lagged_dataset = create_lagged_dataset(dataset, lag, target_var)\n",
    "    lagged_dataset_1 = create_lagged_dataset(dataset_1, lag, target_var)\n",
    "    lagged_dataset_2 = create_lagged_dataset(dataset_2, lag, target_var)\n",
    "    \n",
    "    X, y = create_stepped_dataset(lagged_dataset, step, target_var)\n",
    "    X_1, y_1 = create_stepped_dataset(lagged_dataset_1, step, target_var)\n",
    "    X_2, y_2 = create_stepped_dataset(lagged_dataset_2, step, target_var)\n",
    "    \n",
    "    window_start, window_end = create_window(X, window_perc)\n",
    "\n",
    "    print(F'The first epiweek to be predicted for {target_var} lag {lag} step {step} is: {window_end+1}')\n",
    "\n",
    "    ## this is to get naive forecast in the initial trainset for MASE calculation\n",
    "    #### naive forecast and other models have different mechanisms, so the data length is different\n",
    "    if naive.index[0] <= window_start:\n",
    "        y_pred_train_naive = dataset.copy()[[target_var]].loc[window_start:window_end]\n",
    "        y_pred_train_naive.loc[:, 'naive_for_mase'] = np.array(naive.loc[window_start:window_end])\n",
    "    else:\n",
    "        y_pred_train_naive = dataset.copy()[[target_var]].loc[naive.index[0]:window_end]\n",
    "        y_pred_train_naive.loc[:, 'naive_for_mase'] = np.array(naive.loc[:window_end])\n",
    "    pred_train_naive_path = os.path.join(target_var, 'pred_train_naive')\n",
    "    if not os.path.exists(pred_train_naive_path):\n",
    "        os.makedirs(pred_train_naive_path)\n",
    "    pred_train_naive_path = os.path.join(pred_train_naive_path, F'L{lag}_S{step}.csv')\n",
    "    y_pred_train_naive.to_csv(pred_train_naive_path)\n",
    "    \n",
    "    \n",
    "    y_pred = create_output_dataset(y, window_end)\n",
    "    y_params = create_output_dataset(y, window_end)\n",
    "    y_ridge = create_output_dataset(y, window_end)\n",
    "    y_lasso = create_output_dataset(y, window_end)\n",
    "\n",
    "    train_length = len(X.loc[window_start:window_end])\n",
    "    print(F'The initial training dataset length for {target_var} lag {lag} step {step} is: {train_length}')\n",
    "\n",
    "\n",
    "    test_length = len(X.loc[window_end+1:])\n",
    "    print(F'The initial testing dataset length for {target_var} lag {lag} step {step} is: {test_length}')\n",
    "\n",
    "    errors_path = os.path.join(target_var, 'errors', F'L{lag}_S{step}')\n",
    "    pred_train_path = os.path.join(target_var, 'pred_train', F'L{lag}_S{step}')\n",
    "    \n",
    "    if not os.path.exists(errors_path):\n",
    "        os.makedirs(errors_path)\n",
    "    if not os.path.exists(pred_train_path):\n",
    "        os.makedirs(pred_train_path)\n",
    "        \n",
    "    regression_with_naive(X, y, X_1, y_1, X_2, y_2, window_start, window_end, y_pred, y_params, errors_path, test_length, naive, history_mean, target_var, y_ridge, y_lasso, pred_train_path, lag)\n",
    "\n",
    "    pred_path = os.path.join(target_var, 'pred')\n",
    "    params_path = os.path.join(target_var, 'params')\n",
    "    \n",
    "    ridge_path = os.path.join(target_var, 'ridge_param')\n",
    "    lasso_path = os.path.join(target_var, 'lasso_param')\n",
    "\n",
    "    '''\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "    if not os.path.exists(params_path):\n",
    "        os.makedirs(params_path)\n",
    "    '''\n",
    "    if not os.path.exists(ridge_path):\n",
    "        os.makedirs(ridge_path)\n",
    "    if not os.path.exists(lasso_path):\n",
    "        os.makedirs(lasso_path)\n",
    "\n",
    "    pred_path = os.path.join(pred_path, F'L{lag}_S{step}.csv')\n",
    "    params_path = os.path.join(params_path, F'L{lag}_S{step}.csv')\n",
    "    ridge_path = os.path.join(ridge_path, F'L{lag}_S{step}.csv')\n",
    "    lasso_path = os.path.join(lasso_path, F'L{lag}_S{step}.csv')    \n",
    "\n",
    "\n",
    "    \n",
    "    y_pred.to_csv(pred_path)\n",
    "    y_params.to_csv(params_path)\n",
    "\n",
    "    y_ridge.to_csv(ridge_path)\n",
    "    y_lasso.to_csv(lasso_path)\n",
    "    \n",
    "\n",
    "    print(F'Completed for {target_var} lag {lag} step {step}')\n",
    "    clear_output(wait=False)\n",
    "    return y_ridge, y_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d5bb5-c6e2-44dd-bab9-fb440ba32173",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function runs the regression for one disease, for all lags and steps, hence the for loop\n",
    "\n",
    "def run_disease_regression(dataset, disease_var, lag_start, lag_end, step_start, step_end):\n",
    "    \n",
    "    ## Note how the integer disease_var is input into this function, and then\n",
    "    ## the string target_var is returned for the remaining functions\n",
    "    explore_df, explore_df_1, explore_df_2, target_var = create_initial_dataset(dataset, disease_var)\n",
    "\n",
    "    with open(\"target_variables.txt\") as target_variables_file:\n",
    "        if target_var not in target_variables_file.read():\n",
    "            with open(\"target_variables.txt\", 'a') as target_variables_file:\n",
    "                target_variables_file.write(F'{target_var}\\n')\n",
    "    \n",
    "    ## run the first order regression for all lags and steps for this target variable\n",
    "    print(F'Running regression for {target_var}')\n",
    "    for lag in range(lag_start, lag_end):\n",
    "        for step in range(step_start, step_end):\n",
    "            run_first_order_regression(explore_df, explore_df_1, explore_df_2, lag = lag, step = step, target_var = target_var, window_perc = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787b52d-4052-4a42-9be8-b5e233ea441e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## x in range (0,16) represents the 16 diseases that are the target variables. However, for this function we input them as integers\n",
    "## the create_initial_dataset function will convert the integer format to string format\n",
    "## Using parallel, each disease can be run on one computer core\n",
    "Parallel(n_jobs=-2, verbose=51)(delayed(run_disease_regression)(weatherclimateED, x, 8, 9, 1, 13) for x in range(0,16))\n",
    "#run_full_regression(weatherclimateED, range(0,16), 8, 9, 1, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912e0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
