{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a344836-2055-4318-b7ca-62f093c5cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04_2 CSR and Random projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f433e-8aff-4fcb-8a4b-697e5b57f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "weatherclimateED = pd.read_csv('weatherclimateED.csv', parse_dates = [0], dayfirst = True)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import combinations\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d1977-f012-4005-9a58-dd6b010c27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Epiweeks Module converts dates to CDC Epiweek format\n",
    "## Further documentation on https://pypi.org/project/epiweeks/\n",
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "def create_epiweek(date):\n",
    "    return Week.fromdate(date)\n",
    "def create_epiweekplot(epiweek):\n",
    "    epiweek = str(epiweek)\n",
    "    return F'Y{epiweek[:4]}W{epiweek[4:]}'\n",
    "def create_epiweek_fromstr(str):\n",
    "    return Week.fromstring(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f4c3f-fbee-4b7f-8197-c3948ab6c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This section creates a full complete dataset that includes all the variables of interest that will be used\n",
    "## iloc function selects the relevant variables of interest based on column number\n",
    "## Problematic weather columns (i.e. don't select!): 6, 16, 17, 19, 20\n",
    "## Disease columns excluded due to limited dataset: 21:24, 25\n",
    "\n",
    "weatherclimateED['epiweek'] = weatherclimateED['Date'].apply(create_epiweek)\n",
    "weatherclimateED = weatherclimateED.set_index('epiweek')\n",
    "weatherclimateED = weatherclimateED.iloc[:, np.r_[30:32, 33:39, 40, 42 , 45:47, 49:51,  52:54, 1:6, 8:15]]\n",
    "weatherclimateED.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7fdf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_by_index(lst, indices):\n",
    "    return [item for idx, item in enumerate(lst) if idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f3296-097d-40b0-a7c2-6d598a519f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function takes the full dataset and creates an initial dataset with the specified range\n",
    "## also returns the name of the target variable for creation of the initial dataset\n",
    "## note disease_var here is an integer based off the column number\n",
    "## we call the number of exogenous predictors, P. Pick p = 1, 2, ..., 5\n",
    "def create_initial_dataset(dataset, disease_var: int, P, p):\n",
    "    target_disease_order = disease_var # disease_var is a number\n",
    "    explore_df = dataset.copy()\n",
    "    range_start = Week(2009,1)\n",
    "    range_end = Week (2018,52)\n",
    "    explore_df = explore_df.loc[range_start:range_end]\n",
    "    target_var = explore_df.columns.values.tolist()[disease_var]\n",
    "     \n",
    "    ## For P8\n",
    "    combs = list(combinations(range(16, 16 + P), p))\n",
    "    eds_list = list(range(0, target_disease_order)) + list(range(target_disease_order+1, 16))\n",
    "    combs_eds = list(combinations(eds_list, p))\n",
    "    \n",
    "    new_explore_dfs = []\n",
    "\n",
    "    for comb in combs:\n",
    "        for comb_eds in combs_eds:\n",
    "            new_explore_df_parts = [explore_df[[explore_df.columns[target_disease_order]]]]\n",
    "            for i in range(p):\n",
    "                column_selected = explore_df.iloc[:, [comb[i], comb_eds[i]]]\n",
    "                new_explore_df_parts.append(column_selected)\n",
    "            new_explore_df = pd.concat(new_explore_df_parts, axis=1)\n",
    "            new_explore_dfs.append(new_explore_df)\n",
    "    \n",
    "    if p > 1:\n",
    "        keep_index_list = np.random.choice(len(new_explore_dfs), size=1000, replace=False)\n",
    "        new_explore_dfs = keep_by_index(new_explore_dfs, keep_index_list)    \n",
    "    \n",
    "    return explore_df, new_explore_dfs, target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b1deb-e97f-45d7-b5f7-6c4f296caea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged dataset\n",
    "def create_lagged_dataset(dataset, lag, target_var):\n",
    "    lagged_dataset = dataset.copy()\n",
    "    columns_list = list(lagged_dataset.columns)\n",
    "    data_join = {}\n",
    "    for column in columns_list:\n",
    "        if column == target_var:\n",
    "            data_join[column] = lagged_dataset[column]\n",
    "        for n in range(1,lag+1):\n",
    "            data_join[F'{column}_L{n}'] = lagged_dataset[column].shift(n)\n",
    "    lagged_dataset = pd.concat(data_join.values(), axis=1, ignore_index = True)\n",
    "    lagged_dataset.columns = data_join.keys()\n",
    "    return lagged_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672d89c-abc7-4fa5-8e1b-5c6bfc8fffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step is the number of weeks ahead that we are forecasting, e.g. step=2 is 2 weeks ahead.\n",
    "## Note step=1 results in no change to dataset, i.e. use generated lagged variables to forecast current. \n",
    "def create_stepped_dataset(dataset, step, target_var):\n",
    "    stepped_dataset = dataset.copy()\n",
    "    y = stepped_dataset[[target_var]].shift(-step+1)\n",
    "    if step != 1:\n",
    "        X = stepped_dataset.iloc[:-step+1, :]\n",
    "    else:\n",
    "        X = stepped_dataset\n",
    "    return X.drop(target_var, axis = 1), y.dropna()\n",
    "## So now target variable (y variable for exploration) is shifted back by 2 weeks. i.e., taking the y-value from 2 weeks later\n",
    "## and setting it to the current index. So linear regression of y+2 with the current X values. X will have\n",
    "## a smaller dataset with the last 2 time points removed because of the shift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112b902-bf3c-426a-ac21-d98a3bd3a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(X, window_perc):\n",
    "    return X.index[0], X.index[int(len(X)*window_perc)]\n",
    "def create_output_dataset(y, window_end):\n",
    "    return y.copy().loc[window_end+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f8bdf-0d86-42b3-a937-6094d334ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from group_lasso import GroupLasso\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "# from nixtlats import TimeGPT\n",
    "from sklearn.decomposition import PCA\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import globalenv\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import Formula\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "\n",
    "def coefs(model, coefs_path, filename):\n",
    "    coefs_path\n",
    "\n",
    "## This function runs the first order regression for the target disease, for one specified lag and step\n",
    "\n",
    "def regression_comb(X_dataset, y_dataset, window_start, window_end, y_pred, test_length, target_var, lag):\n",
    "    count = 0\n",
    "    df_end = X_dataset.index[-1]\n",
    "    while window_end != df_end:\n",
    "        X = X_dataset.copy()\n",
    "        y = y_dataset.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train = X.loc[window_start:window_end]\n",
    "        #print(X_train.info())\n",
    "        ## values.ravel() converts y_train to numpy array for compatibility with models (update: already deleted this)\n",
    "        y_train = y.loc[window_start:window_end]\n",
    "        #print(len(y_train))\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test = X.loc[[window_end+1]]\n",
    "        #print(X_test)\n",
    "        y_test = y.loc[window_end+1]\n",
    "        #print(y_test)\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # For all models using all variables, I use dataframe and not array\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.columns = X.columns\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = X.columns\n",
    "\n",
    "\n",
    "        ## P8 CSL p = 1, 2, 3\n",
    "        csl = LinearRegression()\n",
    "        csl.fit(X_train, y_train)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'CSL'] = csl.predict(X_test)\n",
    "\n",
    "        \n",
    "\n",
    "        ##\n",
    "        #keep track of model progress, every number of weeks\n",
    "        tracking_interval = 5\n",
    "#         if window_end.weektuple()[1] % tracking_interval == 0:\n",
    "#             print(F'{target_var} done with {window_end+1}; {count} out of {test_length}')\n",
    "            \n",
    "        ## Implement expanding window\n",
    "        #window_start = window_start+1 (only for rolling window)\n",
    "        window_end += 1\n",
    "        count += 1\n",
    "\n",
    "#     print(F'The last epiweek for {target_var} to be predicted is: {window_end}')\n",
    "#     print(F'The total number of predicted epiweeks for {target_var} is: {count}')\n",
    "    \n",
    "\n",
    "def regression_comb_1(X_dataset, y_dataset, window_start, window_end, y_pred, test_length, target_var, lag):\n",
    "    count = 0\n",
    "    df_end = X_dataset.index[-1]\n",
    "    while window_end != df_end:\n",
    "        X = X_dataset.copy()\n",
    "        y = y_dataset.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train = X.loc[window_start:window_end]\n",
    "        #print(X_train.info())\n",
    "        ## values.ravel() converts y_train to numpy array for compatibility with models (update: already deleted this)\n",
    "        y_train = y.loc[window_start:window_end]\n",
    "        #print(len(y_train))\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test = X.loc[[window_end+1]]\n",
    "        #print(X_test)\n",
    "        y_test = y.loc[window_end+1]\n",
    "        #print(y_test)\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # For all models using all variables, I use dataframe and not array\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.columns = X.columns\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = X.columns\n",
    "\n",
    "\n",
    "        ## P9 RP p = 1, 2, 3\n",
    "        rp = LinearRegression()\n",
    "        rp.fit(X_train, y_train)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'RP'] = rp.predict(X_test)\n",
    "\n",
    "        \n",
    "\n",
    "        ##\n",
    "        #keep track of model progress, every number of weeks\n",
    "        tracking_interval = 5\n",
    "#         if window_end.weektuple()[1] % tracking_interval == 0:\n",
    "#             print(F'{target_var} done with {window_end+1}; {count} out of {test_length}')\n",
    "            \n",
    "        ## Implement expanding window\n",
    "        #window_start = window_start+1 (only for rolling window)\n",
    "        window_end += 1\n",
    "        count += 1\n",
    "\n",
    "#     print(F'The last epiweek for {target_var} to be predicted is: {window_end}')\n",
    "#     print(F'The total number of predicted epiweeks for {target_var} is: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12edf56-31b5-4b42-a957-7df729f4892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function sets up the first order regression for the target disease, for one specified lag and step\n",
    "\n",
    "def run_regression_comb(dataset, new_datasets, lag, step, target_var, window_perc, p):\n",
    "    print(F'Running regression combination for {target_var} lag {lag} step {step}')\n",
    "    \n",
    "    ## For P8\n",
    "    preds = []\n",
    "    for new_dataset in new_datasets:\n",
    "        lagged_dataset = create_lagged_dataset(new_dataset, lag, target_var)\n",
    "\n",
    "        X, y = create_stepped_dataset(lagged_dataset, step, target_var)\n",
    "\n",
    "        window_start, window_end = create_window(X, window_perc)\n",
    "\n",
    "#         print(F'The first epiweek to be predicted for {target_var} lag {lag} step {step} is: {window_end+1}')\n",
    "\n",
    "        y_pred = create_output_dataset(y, window_end)\n",
    "\n",
    "        train_length = len(X.loc[window_start:window_end])\n",
    "#         print(F'The initial training dataset length for {target_var} lag {lag} step {step} is: {train_length}')\n",
    "\n",
    "\n",
    "        test_length = len(X.loc[window_end+1:])\n",
    "#         print(F'The initial testing dataset length for {target_var} lag {lag} step {step} is: {test_length}')\n",
    "\n",
    "\n",
    "        regression_comb(X, y, window_start, window_end, y_pred, test_length, target_var, lag)\n",
    "        preds.append(y_pred)\n",
    "    average_pred = pd.concat(preds, axis=0).groupby(level=0).mean() \n",
    "    \n",
    "    \n",
    "    ## For P9\n",
    "    lagged_dataset = create_lagged_dataset(dataset, lag, target_var)\n",
    "\n",
    "    X, y = create_stepped_dataset(lagged_dataset, step, target_var)\n",
    "\n",
    "    window_start, window_end = create_window(X, window_perc)\n",
    "    \n",
    "    train_length = len(X.loc[window_start:window_end])\n",
    "\n",
    "    test_length = len(X.loc[window_end+1:])\n",
    "    \n",
    "    target_start = 0\n",
    "    for count, column in enumerate(X.columns):\n",
    "        if column[0:-3] == target_var:\n",
    "            target_start = count\n",
    "            break\n",
    "    target_end = target_start + 8\n",
    "    \n",
    "    X_env = X[X.columns[128:].to_list()]\n",
    "    X_only_target = X[X.columns[target_start:target_end].to_list()]\n",
    "    eds_columns = X.columns[0:target_start].to_list() + X.columns[target_end:128].to_list()\n",
    "    X_eds = X[eds_columns]\n",
    "    nrow_matrix_env = np.array(X_env).shape[1]\n",
    "    nrow_matrix_eds = np.array(X_eds).shape[1]\n",
    "    ncol_matrix = p\n",
    "    num_matrices = 100\n",
    "    # np.random.seed(123)\n",
    "    matrices_env = [np.random.normal(0, 1, (nrow_matrix_env, ncol_matrix)) for _ in range(num_matrices)]\n",
    "    matrices_eds = [np.random.normal(0, 1, (nrow_matrix_eds, ncol_matrix)) for _ in range(num_matrices)]\n",
    "\n",
    "    # sampling 1000 from 100 * 100\n",
    "    all_combinations = [(x, y) for x in range(num_matrices) for y in range(num_matrices)]\n",
    "    sampled_tuples = random.sample(all_combinations, 1000)\n",
    "\n",
    "    new_X_sets = []\n",
    "    for tuple in sampled_tuples:\n",
    "        X_proj_env = pd.DataFrame(np.dot(np.array(X_env), matrices_env[tuple[0]]))\n",
    "        X_proj_env.columns = X_proj_env.columns.astype(str)\n",
    "        X_proj_env.index = X.index\n",
    "\n",
    "        X_proj_eds = pd.DataFrame(np.dot(np.array(X_eds), matrices_eds[tuple[1]]))\n",
    "        X_proj_eds.columns = X_proj_eds.columns.astype(str)\n",
    "        X_proj_eds.index = X.index\n",
    "\n",
    "        new_X_sets.append(pd.concat([X_only_target, X_proj_env, X_proj_eds], axis=1))\n",
    "    \n",
    "    preds_1 = []\n",
    "    for new_X in new_X_sets:\n",
    "        y_pred_1 = create_output_dataset(y, window_end)\n",
    "        regression_comb_1(new_X, y, window_start, window_end, y_pred_1, test_length, target_var, lag)\n",
    "        preds_1.append(y_pred_1)\n",
    "    average_pred_1 = pd.concat(preds_1, axis=0).groupby(level=0).mean()\n",
    "    average_pred_1.drop(columns=target_var, inplace=True)\n",
    "    \n",
    "    final_pred = pd.concat([average_pred, average_pred_1], axis=1)\n",
    "    \n",
    "    pred_path = os.path.join(target_var, f'pred_csr_rp_{p}')\n",
    "\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "\n",
    "    pred_path = os.path.join(pred_path, F'L{lag}_S{step}.csv')\n",
    "\n",
    "\n",
    "    final_pred.to_csv(pred_path)\n",
    "    \n",
    "\n",
    "    print(F'Completed for {target_var} lag {lag} step {step} p {p}')\n",
    "    clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d5bb5-c6e2-44dd-bab9-fb440ba32173",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function runs the regression for one disease, for all lags and steps, hence the for loop\n",
    "\n",
    "def run_disease_regression_comb(dataset, disease_var, lag_start, lag_end, step_start, step_end, P):\n",
    "    \n",
    "    ## Note how the integer disease_var is input into this function, and then\n",
    "    ## the string target_var is returned for the remaining functions\n",
    "    \n",
    "    for p in range(1,4):\n",
    "        explore_df, new_explore_dfs, target_var = create_initial_dataset(dataset, disease_var, P, p)\n",
    "\n",
    "        with open(\"target_variables.txt\") as target_variables_file:\n",
    "            if target_var not in target_variables_file.read():\n",
    "                with open(\"target_variables.txt\", 'a') as target_variables_file:\n",
    "                    target_variables_file.write(F'{target_var}\\n')\n",
    "\n",
    "        ## run the first order regression for all lags and steps for this target variable\n",
    "        print(F'Running regression for {target_var}')\n",
    "        for lag in range(lag_start, lag_end):\n",
    "            for step in range(step_start, step_end):\n",
    "                run_regression_comb(explore_df, new_explore_dfs, lag = lag, step = step, target_var = target_var, window_perc = 0.7, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787b52d-4052-4a42-9be8-b5e233ea441e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Main function call using Parallel\n",
    "## x in range (0,16) represents the 16 diseases that are the target variables. However, for this function we input them as integers\n",
    "## the create_initial_dataset function will convert the integer format to string format\n",
    "## Using parallel, each disease can be run on one computer core\n",
    "np.random.seed(123)\n",
    "Parallel(n_jobs=-2, verbose=51)(delayed(run_disease_regression_comb)(weatherclimateED, x, 8, 9, 1, 13, 12) for x in range(0,16))\n",
    "#run_full_regression(weatherclimateED, range(0,16), 8, 9, 1, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801aa1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
