{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e0739-63a1-41a0-b141-0c14b4fd16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#05 Second Order Regression\n",
    "## This code takes the prediction forecast output from 03Autoregression and runs a second order regression\n",
    "## In the first order (03IndividualModels) case: disease and weather variables were the X values for predicting Y - target variable\n",
    "## In the second order case (this one): prediction forecasts from first order, i.e., Naive Forecast, Linear Regression, LASSO, Ridge, etc\n",
    "## are the new X values for predicting Y the target variable. \n",
    "\n",
    "## During this second order regression, we run the same forecast methods, e.g. Linear, RandomForest, etc, a second time\n",
    "## In the same way that in first order regression the forecast methods help us choose which disease/weather variables best predict the target variable,\n",
    "## In second order regression, the forecast methods help us choose which forecast methods best predict the target variable. \n",
    "\n",
    "## The code's structure here is almost identical to 03.ipynb, but it takes in the inputs from 03.ipynb prediction outputs.\n",
    "## Additionally, naive forecast method is omitted from second order. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36da9eb-5739-4339-a3b8-33921b69b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import globalenv\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import Formula\n",
    "from rpy2.robjects.conversion import localconverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358166ea-9279-469c-b7ad-61ab5fe36aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "def create_epiweek(date):\n",
    "    return Week.fromdate(date)\n",
    "def create_epiweekplot(epiweek):\n",
    "    epiweek = str(epiweek)\n",
    "    return F'Y{epiweek[:4]}W{epiweek[4:]}'\n",
    "def create_epiweek_fromstr(str):\n",
    "    return Week.fromstring(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77b682-8966-4400-a03c-6bd96ebe6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_and_y(dataset, target_var):\n",
    "    X_and_y = dataset.copy()\n",
    "    return X_and_y.drop(target_var, axis = 1), X_and_y[[target_var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5c670-b9f2-4db7-a472-9ed315465772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(X, window_perc):\n",
    "    return X.index[0], X.index[int(len(X)*window_perc)]\n",
    "def create_output_dataset(y, window_end):\n",
    "    return y.copy().loc[window_end+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c59ce0-3930-4ddf-9ee6-fc1040d2bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "def regression_without_naive(X_dataset, y_dataset, window_start, window_end, y_pred, y_params, aenet_weights, test_length):\n",
    "    count = 0\n",
    "    df_end = X_dataset.index[-1]\n",
    "    while window_end != df_end:\n",
    "        X = X_dataset.copy()\n",
    "        y = y_dataset.copy()\n",
    "        # Note: .loc is end-inclusive    \n",
    "        X_train = X.loc[window_start:window_end]\n",
    "        #print(X_train.info())\n",
    "        ## values.ravel() converts y_train to numpy array for compatibility with models\n",
    "        y_train = y.loc[window_start:window_end]\n",
    "        #print(len(y_train))\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_test = X.loc[[window_end+1]]\n",
    "        X_test_beofre_norm = X_test    # This is for P5 (3) \n",
    "        #print(X_test)\n",
    "        y_test = y.loc[window_end+1]\n",
    "        \n",
    "        ## To implement P5 (3), reconstruct the dataset.\n",
    "        X_new_list = []\n",
    "        for i, mod in enumerate(X.columns):\n",
    "            if i == 0:\n",
    "                y_new = pd.DataFrame(y.values - X[[X.columns[0]]].values, columns=y.columns)\n",
    "                y_new.index = y.index\n",
    "            else:\n",
    "                column_new = pd.DataFrame(X[[X.columns[i]]].values - X[[X.columns[0]]].values, columns=[mod])\n",
    "                column_new.index = X.index\n",
    "                X_new_list.append(column_new)\n",
    "\n",
    "        X_new = pd.concat(X_new_list, axis=1)\n",
    "        \n",
    "        X_new_train = X_new.loc[window_start:window_end]\n",
    "        #print(X_train.info())\n",
    "        ## values.ravel() converts y_train to numpy array for compatibility with models\n",
    "        y_new_train = y_new.loc[window_start:window_end].values.ravel()\n",
    "        #print(len(y_train))\n",
    "        ## double square brackets so X_test is extracted as a pandas df instead of series\n",
    "        X_new_test = X_new.loc[[window_end+1]]\n",
    "        #print(X_test)\n",
    "        y_new_test = y_new.loc[window_end+1]\n",
    "    \n",
    "        ## Scaling\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.columns = X.columns\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = X.columns\n",
    "\n",
    "        ## Scaling for X_new (对于X_new, 我保留了标准化后的np.array格式)\n",
    "        scaler = StandardScaler()\n",
    "        ## .fit_transform stores the scaling parameters (fit), and transforms the training set\n",
    "        X_new_train = scaler.fit_transform(X_new_train)\n",
    "        ## .transform takes the previously stored scaling parameters to transform the test set\n",
    "        ## Therefore, test set is transformed based on the training set parameters\n",
    "        X_new_test = scaler.transform(X_new_test)\n",
    "        \n",
    "        ## evaluate variance\n",
    "    \n",
    "        ## Naive Forecast N/A for second order regression\n",
    "        # add the [0] to extract as float, and not as series\n",
    "        #y_pred.at[window_end+1, 'naive'] = naive.loc[window_end+1][0]\n",
    "        \n",
    "        ## P5 (1): Linear Regression Model\n",
    "        linreg_model_1 = LinearRegression()\n",
    "        # Fit the model to the training data\n",
    "        linreg_model_1.fit(X_train, y_train)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'linreg_c_1'] = linreg_model_1.predict(X_test)\n",
    "        \n",
    "        ## P5 (2): Linear Regression Model with no constraints on coef and no intercept\n",
    "        linreg_model_2 = LinearRegression(fit_intercept=False)\n",
    "        # Fit the model to the training data\n",
    "        linreg_model_2.fit(X_train, y_train)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'linreg_c_2'] = linreg_model_2.predict(X_test)\n",
    "        \n",
    "        ## P5 (3): Linear Regression Model with all coefs sum to 1\n",
    "        linreg_model_3 = LinearRegression()\n",
    "        # Fit the model to the training data\n",
    "        linreg_model_3.fit(X_new_train, y_new_train)\n",
    "        # Make predictions and store\n",
    "        y_pred.at[window_end+1, 'linreg_c_3'] = linreg_model_3.predict(X_new_test) + X_test_beofre_norm.iloc[0, 0]\n",
    "\n",
    "\n",
    "#         ## Implement cross-validation split\n",
    "#         tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "#         ## ElasticNet Model\n",
    "#         elasticnet_cv = ElasticNetCV(cv = tscv, max_iter = 100000)\n",
    "#         elasticnet_cv.fit(X_train, y_train)\n",
    "    \n",
    "#         # Create the ElasticNet model with the optimal l1 and alpha values\n",
    "#         elasticnet_model = ElasticNet(alpha = elasticnet_cv.alpha_, l1_ratio = elasticnet_cv.l1_ratio_)\n",
    "#         elasticnet_model.fit(X_train, y_train)\n",
    "#         y_pred.at[window_end+1, 'elasticnet_2'] = elasticnet_model.predict(X_test)\n",
    "#         y_params.at[window_end+1, 'elasticnet_2_alpha'] = elasticnet_cv.alpha_\n",
    "#         y_params.at[window_end+1, 'elasticnet_2_l1ratio'] = elasticnet_cv.l1_ratio_\n",
    "        \n",
    "        \n",
    "        ## P6: Adaptive ElasticNet Model\n",
    "        anent_model = LinearRegression() # first fit a normal lm\n",
    "        anent_model.fit(X_train, y_train)\n",
    "        \n",
    "        pandas2ri.activate()\n",
    "\n",
    "        glmnet = importr('glmnet')\n",
    "        base = importr('base')\n",
    "        base.set_seed(123)\n",
    "\n",
    "        X_r_train = pandas2ri.py2rpy(X_train)\n",
    "        y_r_train = pandas2ri.py2rpy(y_train)\n",
    "\n",
    "        # Convert to matrices in R\n",
    "        ro.r.assign('X_r_train', X_r_train)\n",
    "        ro.r.assign('y_r_train', y_r_train)\n",
    "        ro.r('X_r_train <- as.matrix(X_r_train)')\n",
    "        ro.r('y_r_train <- as.matrix(y_r_train)')\n",
    "\n",
    "        # Fit a linear model (without intercept, adjust according to your needs)\n",
    "        ro.r('model <- lm(y_r_train ~ X_r_train)')  # Adjusted to omit the intercept\n",
    "\n",
    "        # Extract coefficients\n",
    "        ro.r('coefficients <- as.numeric(coef(model))[-1]')  # Adjust if intercept is included\n",
    "\n",
    "        # Perform adaptive LASSO with cross-validation\n",
    "        ro.r('''\n",
    "        aenet_cv <- cv.glmnet(x = X_r_train, y = y_r_train,\n",
    "                                type.measure = \"mse\",\n",
    "                                nfold = 10,\n",
    "                                alpha = 0.5,\n",
    "                                penalty.factor = 1 / (abs(coefficients) + 1e-5),\n",
    "                                keep = TRUE)\n",
    "        ''')\n",
    "\n",
    "        # Extract best coefficients\n",
    "        ro.r('best_aenet_coef <- coef(aenet_cv, s = aenet_cv$lambda.min)')\n",
    "        ro.r('best_aenet_coef <- as.numeric(best_aenet_coef)')\n",
    "        ro.r('penalty_factor <- as.numeric(1 / (abs(coefficients) + 1e-5))')\n",
    "        ro.r('aenet_lambda <- as.numeric(aenet_cv$lambda.min)')\n",
    "\n",
    "        # Retrieve the best coefficients back into Python\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            best_aenet_coef = ro.conversion.rpy2py(ro.globalenv['best_aenet_coef'])\n",
    "            penalty_factor = ro.conversion.rpy2py(ro.globalenv['penalty_factor'])\n",
    "            aenet_lambda = ro.conversion.rpy2py(ro.globalenv['aenet_lambda'])\n",
    "        \n",
    "        \n",
    "        anent_model.coef_ = best_aenet_coef[1:].reshape(1, X_train.shape[1])\n",
    "        anent_model.intercept_ = np.array([best_aenet_coef[0]])\n",
    "        y_pred.at[window_end+1, 'aenet_2'] = anent_model.predict(X_test)\n",
    "        \n",
    "        # store the weights (penalty factors) and lambda\n",
    "        for i, mod in enumerate(X_train.columns):\n",
    "            y_params.at[window_end+1, f'pf_{mod}'] = penalty_factor[i]\n",
    "            aenet_weights.at[window_end+1, f'{mod}_scaled_coef'] = (anent_model.coef_[0][i] / np.abs(anent_model.coef_[0]).sum()) + 1e-5 # rescale the coefs of aenet\n",
    "        y_params.at[window_end+1, 'aenet_2_lambda'] = aenet_lambda\n",
    "        \n",
    "        \n",
    "        ## P7: Random Forest\n",
    "        randomforest_model = RandomForestRegressor(n_estimators = 1000, max_features = 'sqrt', random_state = 18)\n",
    "        randomforest_model.fit(X_train, y_train)\n",
    "        y_pred.at[window_end+1, 'randomforest_2'] = randomforest_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        \n",
    "        ##\n",
    "        #keep track of model progress, every number of weeks\n",
    "        tracking_interval = 5\n",
    "        if window_end.weektuple()[1] % tracking_interval == 0:\n",
    "            print(F'done with {window_end+1}; {count} out of {test_length}')\n",
    "        \n",
    "        ## Implement expanding window\n",
    "        #window_start = window_start+1 (only for rolling window)\n",
    "        window_end += 1\n",
    "        count += 1\n",
    "\n",
    "    print(F'The last epiweek to be predicted is: {window_end}')\n",
    "    print(F'The total number of predicted epiweeks is: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccac213-7abe-4ac1-99b6-8c87d6beec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_regression(dataset, target_var, window_perc):\n",
    "    #print('Running for lag '+str(lag)+' step '+str(step))\n",
    "\n",
    "    #no naive for second order regression\n",
    "    #naive = create_naive(dataset, target_var)\n",
    "    #print(naive.info())\n",
    "    \n",
    "    #lagged_dataset = create_lagged_dataset(dataset, lag)\n",
    "    \n",
    "    X, y = create_X_and_y(dataset, target_var)\n",
    "    print(X.info())\n",
    "    print(y.info())\n",
    "    \n",
    "    window_start, window_end = create_window(X, window_perc)\n",
    "    print(F'The first epiweek to be predicted is: {window_end+1}')\n",
    "    \n",
    "    y_pred = create_output_dataset(y, window_end)\n",
    "    y_params = create_output_dataset(y, window_end)\n",
    "    aenet_weights = create_output_dataset(y, window_end)\n",
    "    \n",
    "    train_length = len(X.loc[window_start:window_end])\n",
    "    print(F'The initial training dataset length is: {train_length}')\n",
    "    test_length = len(X.loc[window_end+1:])\n",
    "    print(F'The initial testing dataset length is: {test_length}')\n",
    "\n",
    "    regression_without_naive(X, y, window_start, window_end, y_pred, y_params, aenet_weights, test_length)\n",
    "    #print('Completed for lag '+str(lag)+' step '+str(step))\n",
    "    clear_output(wait=False)\n",
    "    return y_pred, y_params, aenet_weights.drop(columns=[target_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90150674-abb0-40ae-bc97-879fc523fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_second_order_regression(target_var, pred_directory):\n",
    "    directory = os.path.join(target_var, pred_directory)\n",
    "    for filename in os.listdir(directory):\n",
    "        pred_file = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(pred_file):\n",
    "            print(pred_file)\n",
    "            \n",
    "            y_pred = pd.read_csv(pred_file, parse_dates = [0], dayfirst = True)\n",
    "            y_pred['epiweek'] = y_pred['epiweek'].apply(create_epiweek_fromstr)\n",
    "            y_pred = y_pred.set_index('epiweek')\n",
    "            y_pred_2, y_params_2, aenet_scaled_coef = second_order_regression(y_pred, target_var, 0.7)\n",
    "\n",
    "\n",
    "            pred_2_path = os.path.join(target_var, 'pred_2')\n",
    "            if not os.path.exists(pred_2_path):\n",
    "                os.makedirs(pred_2_path)\n",
    "            y_pred_2.to_csv(os.path.join(pred_2_path, filename))\n",
    "\n",
    "            params_2_path = os.path.join(target_var, 'params_2')\n",
    "            if not os.path.exists(params_2_path):\n",
    "                os.makedirs(params_2_path)\n",
    "            y_params_2.to_csv(os.path.join(params_2_path, filename))\n",
    "            \n",
    "            aenet_weights_path = os.path.join(target_var, 'aenet_weights')\n",
    "            if not os.path.exists(aenet_weights_path):\n",
    "                os.makedirs(aenet_weights_path)\n",
    "            aenet_scaled_coef.to_csv(os.path.join(aenet_weights_path, filename))\n",
    "\n",
    "            print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42f9f0-5a3e-4ff2-babb-f2b7b350d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_second_order_regression(target_variables_file, pred_directory):\n",
    "    target_variables = []\n",
    "    with open(target_variables_file, 'r') as file:\n",
    "        for line in file:\n",
    "            # Remove linebreak which is the last character of the string\n",
    "            target_variable = line[:-1]\n",
    "            # Add item to the list\n",
    "            target_variables.append(target_variable)\n",
    "    print(target_variables)\n",
    "\n",
    "    Parallel(n_jobs=-2, verbose=51)(delayed(run_second_order_regression)(target_var, pred_directory) for target_var in target_variables)\n",
    "    \n",
    "full_second_order_regression('target_variables.txt', 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6de2f6-f09a-4865-a396-4f051c49d080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
